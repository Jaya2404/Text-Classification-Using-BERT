{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jaya2404/Text-Classification-Using-BERT/blob/main/TextClassificationModel.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get the data ready"
      ],
      "metadata": {
        "id": "fFUGwmh2wNci"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "uOLAnWRfL8sA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c14afa11-869c-4832-c980-1a83e6550973"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install Huggingface Transformers library\n",
        "#!pip install transformers\n",
        "\n",
        "# If you meet problems below, restart your kernel and try this instead\n",
        "!pip install transformers[torch]"
      ],
      "metadata": {
        "id": "Q0-tGAo_MAK6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "54edbd93-c567-4fe0-ff9a-4888a534544b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.37.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch!=1.12.0,>=1.11 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.1.0+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.27.2)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.9.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch!=1.12.0,>=1.11->transformers[torch]) (2.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch!=1.12.0,>=1.11->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch!=1.12.0,>=1.11->transformers[torch]) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "import os\n",
        "from os.path import join\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import random\n",
        "from transformers.file_utils import is_tf_available, is_torch_available, is_torch_tpu_available"
      ],
      "metadata": {
        "id": "pOp2MEE9w2-J"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = pd.read_csv(\"/content/drive/My Drive/Corona_NLP_train.csv\",encoding='latin-1')\n",
        "test_data = pd.read_csv(\"/content/drive/My Drive/Corona_NLP_test.csv\",encoding='latin-1')"
      ],
      "metadata": {
        "id": "V403T6hCFGwO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# New Section"
      ],
      "metadata": {
        "id": "kocxgXD-0D3M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set the seed (very important)\n",
        "seed = 35"
      ],
      "metadata": {
        "id": "lZrlMi1NaRuI"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def set_seed(seed: int):\n",
        "    \"\"\"\n",
        "    Helper function for reproducible behavior to set the seed in ``random``, ``numpy``, ``torch`` and/or ``tf`` (if\n",
        "    installed).\n",
        "\n",
        "    Args:\n",
        "        seed (:obj:`int`): The seed to set.\n",
        "    \"\"\"\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    if is_torch_available():\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        # ^^ safe to call this function even if cuda is not available\n",
        "    if is_tf_available():\n",
        "        import tensorflow as tf\n",
        "\n",
        "        tf.random.set_seed(seed)\n",
        "\n",
        "set_seed(seed)"
      ],
      "metadata": {
        "id": "SpR_StjWaQUF"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the 'OriginalTweet' column as lists of strings\n",
        "train_text = train_data['OriginalTweet'].tolist()\n",
        "test_text = test_data['OriginalTweet'].tolist()\n",
        "\n",
        "# Replace 'Extremely Positive' with 'Positive' and 'Extremely Negative' with 'Negative'\n",
        "train_data['Sentiment'] = train_data['Sentiment'].replace({'Extremely Positive': 'Positive', 'Extremely Negative': 'Negative'})\n",
        "test_data['Sentiment'] = test_data['Sentiment'].replace({'Extremely Positive': 'Positive', 'Extremely Negative': 'Negative'})\n",
        "\n",
        "# Drop rows with 'Neutral' sentiment\n",
        "train_data = train_data[train_data['Sentiment'] != 'Neutral']\n",
        "test_data = test_data[test_data['Sentiment'] != 'Neutral']\n",
        "\n",
        "# Convert 'Positive' to 1 and 'Negative' to 0\n",
        "train_data['Sentiment'] = train_data['Sentiment'].map({'Positive': 1, 'Negative': 0})\n",
        "test_data['Sentiment'] = test_data['Sentiment'].map({'Positive': 1, 'Negative': 0})\n",
        "\n",
        "# Convert the 'Sentiment' column to numpy arrays\n",
        "train_label = train_data['Sentiment'].values\n",
        "test_label = test_data['Sentiment'].values"
      ],
      "metadata": {
        "id": "rWes6HKXxOBw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c45dca18-db1b-4509-face-1bf53c2d700b"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-7-af93dffbef3a>:14: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  train_data['Sentiment'] = train_data['Sentiment'].map({'Positive': 1, 'Negative': 0})\n",
            "<ipython-input-7-af93dffbef3a>:15: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  test_data['Sentiment'] = test_data['Sentiment'].map({'Positive': 1, 'Negative': 0})\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download pretrained BERT Tokenizer\n",
        "from transformers import BertTokenizer\n",
        "# credits to https://www.thepythoncode.com/article/finetuning-bert-using-huggingface-transformers-python\n",
        "model_checkpoint = \"bert-base-cased\"\n",
        "tokenizer = BertTokenizer.from_pretrained(model_checkpoint)"
      ],
      "metadata": {
        "id": "7dhhf784MD-q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6eee17fd-33b6-43ac-b8e0-6b3f07f3bc37"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# download pretrained BERT model\n",
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Your Code Here\n",
        "# Filling the number of labels in this classification task.\n",
        "num_labels = 2\n",
        "model = AutoModelForSequenceClassification.from_pretrained(model_checkpoint, num_labels=num_labels)"
      ],
      "metadata": {
        "id": "1DGZW94uxtde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5679aa-ba24-4ed2-c122-3257b169cd70"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Information of BERT model\n",
        "def get_model_layer(model):\n",
        "  return model.config.num_hidden_layers\n",
        "\n",
        "def get_hidden_size(model):\n",
        "  return model.config.hidden_size"
      ],
      "metadata": {
        "id": "P_Don0G40Z1u"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Move the model to GPU\n",
        "# Revisit the GPU tutorial if you meets error in this cell\n",
        "model = model.to('cuda')"
      ],
      "metadata": {
        "id": "AQpxIgL36kBY"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Metrics\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def compute_metrics(pred):\n",
        "  labels = pred.label_ids\n",
        "  preds = pred.predictions.argmax(-1)\n",
        "  # calculate accuracy using sklearn's function\n",
        "  acc = accuracy_score(labels, preds)\n",
        "  tn, fp, fn, tp = confusion_matrix(labels, preds).ravel()\n",
        "  return {\n",
        "      'accuracy': acc,\n",
        "      \"tn\": tn,\n",
        "      \"fp\": fp,\n",
        "      \"fn\": fn,\n",
        "      \"tp\": tp\n",
        "  }"
      ],
      "metadata": {
        "id": "GKAh_hjXycDl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Change the batch size here\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=3,              # total number of training epochs\n",
        "    per_device_train_batch_size=64,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        "    load_best_model_at_end=True,     # load the best model when finished training (default metric is loss)\n",
        "                                     # but you can specify `metric_for_best_model` argument to change to accuracy or other metric\n",
        "    logging_steps=100,               # log & save weights each logging_steps\n",
        "    save_steps=100,\n",
        "    evaluation_strategy=\"steps\",     # evaluate each `logging_steps`?\n",
        ")\n"
      ],
      "metadata": {
        "id": "4TCYaNha1xPO"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Encode the data\n",
        "\n",
        "max_length = 128\n",
        "train_encodings = tokenizer(train_text, truncation=True, padding=True, max_length=max_length)\n",
        "test_encodings = tokenizer(test_text, truncation=True, padding=True, max_length=max_length)"
      ],
      "metadata": {
        "id": "ar0Wn2To2RQf"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Build the dataset\n",
        "\n",
        "class TweetDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {k: torch.tensor(v[idx]) for k, v in self.encodings.items()}\n",
        "        item[\"labels\"] = torch.tensor([self.labels[idx]])\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "# convert our tokenized data into a torch Dataset\n",
        "train_dataset = TweetDataset(train_encodings, train_label)\n",
        "test_dataset = TweetDataset(test_encodings, test_label)"
      ],
      "metadata": {
        "id": "rXYegjcY2aJm"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Setup the trainer\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=train_dataset,         # training dataset\n",
        "    eval_dataset=test_dataset,          # evaluation dataset\n",
        "    compute_metrics=compute_metrics,     # the callback that computes metrics of interest\n",
        ")"
      ],
      "metadata": {
        "id": "outYCdIE2mLy"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# train the model\n",
        "trainer.train()"
      ],
      "metadata": {
        "id": "quxUqKTb2zJG",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 580
        },
        "outputId": "ea731b6c-d081-4810-d333-85601b43eb61"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1569' max='1569' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1569/1569 40:56, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>Tn</th>\n",
              "      <th>Fp</th>\n",
              "      <th>Fn</th>\n",
              "      <th>Tp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.697200</td>\n",
              "      <td>0.697265</td>\n",
              "      <td>0.483800</td>\n",
              "      <td>29</td>\n",
              "      <td>1604</td>\n",
              "      <td>37</td>\n",
              "      <td>1509</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.693600</td>\n",
              "      <td>0.695160</td>\n",
              "      <td>0.484429</td>\n",
              "      <td>993</td>\n",
              "      <td>640</td>\n",
              "      <td>999</td>\n",
              "      <td>547</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>300</td>\n",
              "      <td>0.695200</td>\n",
              "      <td>0.712684</td>\n",
              "      <td>0.486316</td>\n",
              "      <td>0</td>\n",
              "      <td>1633</td>\n",
              "      <td>0</td>\n",
              "      <td>1546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>400</td>\n",
              "      <td>0.696800</td>\n",
              "      <td>0.695161</td>\n",
              "      <td>0.489777</td>\n",
              "      <td>291</td>\n",
              "      <td>1342</td>\n",
              "      <td>280</td>\n",
              "      <td>1266</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>0.694000</td>\n",
              "      <td>0.696961</td>\n",
              "      <td>0.486316</td>\n",
              "      <td>0</td>\n",
              "      <td>1633</td>\n",
              "      <td>0</td>\n",
              "      <td>1546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>600</td>\n",
              "      <td>0.695700</td>\n",
              "      <td>0.698327</td>\n",
              "      <td>0.486316</td>\n",
              "      <td>0</td>\n",
              "      <td>1633</td>\n",
              "      <td>0</td>\n",
              "      <td>1546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>700</td>\n",
              "      <td>0.696700</td>\n",
              "      <td>0.699595</td>\n",
              "      <td>0.486946</td>\n",
              "      <td>3</td>\n",
              "      <td>1630</td>\n",
              "      <td>1</td>\n",
              "      <td>1545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>800</td>\n",
              "      <td>0.692000</td>\n",
              "      <td>0.696408</td>\n",
              "      <td>0.486946</td>\n",
              "      <td>11</td>\n",
              "      <td>1622</td>\n",
              "      <td>9</td>\n",
              "      <td>1537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>900</td>\n",
              "      <td>0.693800</td>\n",
              "      <td>0.699657</td>\n",
              "      <td>0.486316</td>\n",
              "      <td>0</td>\n",
              "      <td>1633</td>\n",
              "      <td>0</td>\n",
              "      <td>1546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>0.691900</td>\n",
              "      <td>0.695284</td>\n",
              "      <td>0.486946</td>\n",
              "      <td>52</td>\n",
              "      <td>1581</td>\n",
              "      <td>50</td>\n",
              "      <td>1496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1100</td>\n",
              "      <td>0.692800</td>\n",
              "      <td>0.698247</td>\n",
              "      <td>0.494181</td>\n",
              "      <td>593</td>\n",
              "      <td>1040</td>\n",
              "      <td>568</td>\n",
              "      <td>978</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1200</td>\n",
              "      <td>0.687900</td>\n",
              "      <td>0.705223</td>\n",
              "      <td>0.491349</td>\n",
              "      <td>378</td>\n",
              "      <td>1255</td>\n",
              "      <td>362</td>\n",
              "      <td>1184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1300</td>\n",
              "      <td>0.688700</td>\n",
              "      <td>0.699779</td>\n",
              "      <td>0.498584</td>\n",
              "      <td>835</td>\n",
              "      <td>798</td>\n",
              "      <td>796</td>\n",
              "      <td>750</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1400</td>\n",
              "      <td>0.684900</td>\n",
              "      <td>0.706140</td>\n",
              "      <td>0.484744</td>\n",
              "      <td>278</td>\n",
              "      <td>1355</td>\n",
              "      <td>283</td>\n",
              "      <td>1263</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>0.681900</td>\n",
              "      <td>0.717064</td>\n",
              "      <td>0.487575</td>\n",
              "      <td>228</td>\n",
              "      <td>1405</td>\n",
              "      <td>224</td>\n",
              "      <td>1322</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=1569, training_loss=0.6916597827062856, metrics={'train_runtime': 2458.7697, 'train_samples_per_second': 40.806, 'train_steps_per_second': 0.638, 'total_flos': 6599614601594880.0, 'train_loss': 0.6916597827062856, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# evaluate the model\n",
        "trainer.evaluate()"
      ],
      "metadata": {
        "id": "C1PvExNNhsEC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "outputId": "3611fe44-503e-446d-9a4d-507098caa11e"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [50/50 00:24]\n",
              "    </div>\n",
              "    "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'eval_loss': 0.6951601505279541,\n",
              " 'eval_accuracy': 0.4844290657439446,\n",
              " 'eval_tn': 993,\n",
              " 'eval_fp': 640,\n",
              " 'eval_fn': 999,\n",
              " 'eval_tp': 547,\n",
              " 'eval_runtime': 24.6384,\n",
              " 'eval_samples_per_second': 129.026,\n",
              " 'eval_steps_per_second': 2.029,\n",
              " 'epoch': 3.0}"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_prediction(model, text):\n",
        "    # prepare our text into tokenized sequence\n",
        "    inputs = tokenizer(text, padding=True, truncation=True, max_length=max_length, return_tensors=\"pt\").to(\"cuda\")\n",
        "    # perform inference to our model\n",
        "    outputs = model(**inputs)\n",
        "    # get output probabilities by doing softmax\n",
        "    probs = outputs[0].softmax(1)\n",
        "    # executing argmax function to get the candidate label\n",
        "    return probs"
      ],
      "metadata": {
        "id": "ue-t1kHWapo0"
      },
      "execution_count": 20,
      "outputs": []
    }
  ]
}